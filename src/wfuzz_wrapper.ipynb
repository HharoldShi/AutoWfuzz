{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class URL:\n",
    "    def __init__(self,url):\n",
    "        self.url = url        \n",
    "        self.getparams = []\n",
    "        self.postparams = []\n",
    "\n",
    "\n",
    "#root_url = \n",
    "\n",
    "class ScrapedURLs:\n",
    "    def __init__(self, root_url):\n",
    "        self.root = root_url\n",
    "        self.urls = {self.root}\n",
    "        self.keywords = set()\n",
    "        self.form_keywords = set()\n",
    "        self.visited = set()\n",
    "        self.forms = dict()\n",
    "        self.get_urls = dict()\n",
    "        self.get_keywords = set()\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.urls = {self.root}\n",
    "        self.keywords = set()\n",
    "        self.form_keywords = set()\n",
    "        self.visited = set()\n",
    "        self.forms = dict()\n",
    "\n",
    "    def scrape_all(self):\n",
    "        if len(self.urls)<=len(self.visited):\n",
    "            return self.output()\n",
    "        else:\n",
    "            temp_urls = set()\n",
    "            temp_keywords = set()\n",
    "            temp_visited = set()\n",
    "\n",
    "            unvisited = self.urls-self.visited\n",
    "\n",
    "            # update keywords\n",
    "            for u in unvisited:\n",
    "                fragments =u[7:].split(\"/\")\n",
    "                fragments =set(fragments)-{\"\"}\n",
    "                if len(fragments | self.keywords) > len(self.keywords):\n",
    "                    temp_keywords.update(fragments)\n",
    "\n",
    "            existing_keywords = temp_keywords | self.keywords\n",
    "\n",
    "            # scrape\n",
    "            for u in unvisited:\n",
    "                fragments =u[7:].split(\"/\")\n",
    "                fragments =set(fragments)-{\"\"}\n",
    "\n",
    "                if len(fragments | self.keywords) > len(self.keywords):\n",
    "                    print(\"Scraping from \"+u)\n",
    "                    url,get_url,form = self.scrape(u)\n",
    "                    if url:\n",
    "                        for i in url:\n",
    "                            #if i.find('?') == -1:                        \n",
    "                            _frag = i[7:].split(\"/\")\n",
    "                            _frag = set(_frag)-{\"\"}\n",
    "\n",
    "                            if len(_frag | existing_keywords) > len(existing_keywords):\n",
    "                                existing_keywords.update(_frag)\n",
    "                                r = i\n",
    "                                if i[-1]=='/':\n",
    "                                    r = i[:-1]\n",
    "                                temp_urls.add(i)\n",
    "                                \n",
    "                    if get_url:\n",
    "                        for i in get_url:\n",
    "                            r= i \n",
    "                            if i[-1]==\"/\":\n",
    "                                r = i[:-1]\n",
    "                            _frag = r[7:].split(\"/\")\n",
    "                            m = _frag[-1]  \n",
    "                            key = r[:(m.find('=')-len(m))]\n",
    "                            if key not in self.get_keywords:\n",
    "\n",
    "                                        self.get_keywords.add(key)\n",
    "\n",
    "                                        self.get_urls.update({r:[]})\n",
    "                            \n",
    "                    if form:\n",
    "                        for key in form:\n",
    "                            r= key[0].rstrip('/') \n",
    "                            _frag = r[7:].split(\"/\")\n",
    "\n",
    "                            if _frag[-1] not in self.form_keywords:\n",
    "\n",
    "                                        self.form_keywords.add(_frag[-1])\n",
    "                                        self.forms.update({r:form[key]})\n",
    "\n",
    "                    temp_visited.add(u)\n",
    "\n",
    "            self.visited.update(temp_visited)\n",
    "            self.urls.update(temp_urls)\n",
    "            self.keywords.update(temp_keywords)\n",
    "\n",
    "            return self.scrape_all()    \n",
    "        \n",
    "    def scrape(self,url):\n",
    "        try:\n",
    "            page = urlopen(url)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print('HTTPError: {}'.format(e.code))\n",
    "            return [], [], {}\n",
    "        html = page.read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        urls = self.find_url(soup,url)\n",
    "        good, get = self.filter_url(urls)\n",
    "        forms = self.find_form(soup,url)\n",
    "        return good, get, forms\n",
    "    \n",
    "    \n",
    "    def find_url(self,soup,url):\n",
    "        if url[-1]!='/':\n",
    "            url = url+'/'\n",
    "\n",
    "        links=soup.find_all(\"a\")\n",
    "        if links == []:\n",
    "            return []\n",
    "        \n",
    "        sub_urls = []\n",
    "        for link in links:\n",
    "            \n",
    "            i = link[\"href\"]\n",
    "            i = re.sub(r\"\\/*\",\"\",i)\n",
    "            if re.match(\"http\",i):\n",
    "                pass\n",
    "            elif re.match(\"mailto:\",i):\n",
    "                pass\n",
    "            else:\n",
    "                sub_urls.append(url+i)\n",
    "        return sub_urls\n",
    "    \n",
    "    def filter_url(self,url_list):\n",
    "        good_urls = []\n",
    "        get_urls = []\n",
    "        for i in url_list:   \n",
    "            t = i.rstrip('/')\n",
    "            _frag = t.split('/')[-1]\n",
    "            if '?' in _frag and '=' in _frag:\n",
    "                get_urls.append(t)\n",
    "            else:\n",
    "                good_urls.append(t)\n",
    "        return good_urls, get_urls        \n",
    "        \n",
    "            \n",
    "    def find_form(self,soup,url):\n",
    "        if url[-1]!='/':\n",
    "            url = url+'/'\n",
    "\n",
    "        forms = soup.find_all('form')\n",
    "        actions = {}\n",
    "\n",
    "        for form in forms:\n",
    "             \n",
    "            try:\n",
    "                actions[(url,form['action'])] = []\n",
    "                \n",
    "            except:\n",
    "                print(\"no action on \"+url)\n",
    "                continue\n",
    "\n",
    "            for element in form.find_all('input'):\n",
    "                if re.search('name',  str(element)):\n",
    "                    actions[(url,form['action'])] .append(element['name'])\n",
    "                    \n",
    "                    #print(element['name'])\n",
    "        return actions   \n",
    "    \n",
    "    def output(self):\n",
    "        output_list = []\n",
    "        allurls = {**self.forms, **self.get_urls}\n",
    "        for i in allurls:\n",
    "            _frag = i.split('/')[-1]\n",
    "            # get\n",
    "            if '?' in _frag and '=' in _frag:\n",
    "                #_frag.split('=')[-1]\n",
    "                #i[:(i.rfind(\"=\"))]\n",
    "\n",
    "                u = URL(i[:(i.rfind(\"?\"))])\n",
    "                s = i.rfind(\"?\")+1\n",
    "                e = - (len(i[s:]) - i[s:].find(\"=\"))\n",
    "                tmp = i[s:e]\n",
    "                u.getparams.append(tmp)\n",
    "                output_list.append(u)\n",
    "                \n",
    "            # post\n",
    "            else:\n",
    "                u = URL(i)\n",
    "                u.postparams = self.forms[i]\n",
    "                output_list.append(u)\n",
    "        return output_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ScrapedURLs('http://testphp.vulnweb.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from http://testphp.vulnweb.com\n",
      "Scraping from http://testphp.vulnweb.com/userinfo.php\n",
      "Scraping from http://testphp.vulnweb.com/login.php\n",
      "Scraping from http://testphp.vulnweb.com/artists.php\n",
      "Scraping from http://testphp.vulnweb.com/cart.php\n",
      "Scraping from http://testphp.vulnweb.com/privacy.php\n",
      "HTTPError: 404\n",
      "Scraping from http://testphp.vulnweb.com/guestbook.php\n",
      "Scraping from http://testphp.vulnweb.com/disclaimer.php\n",
      "Scraping from http://testphp.vulnweb.com/AJAXindex.php\n",
      "HTTPError: 404\n",
      "Scraping from http://testphp.vulnweb.com/Mod_Rewrite_Shop\n",
      "Scraping from http://testphp.vulnweb.com/categories.php\n",
      "Scraping from http://testphp.vulnweb.com/hpp\n",
      "Scraping from http://testphp.vulnweb.com/index.php\n",
      "Scraping from http://testphp.vulnweb.com/userinfo.php/signup.php\n",
      "HTTPError: 302\n",
      "Scraping from http://testphp.vulnweb.com/artists.php/#\n",
      "HTTPError: 404\n",
      "Scraping from http://testphp.vulnweb.com/Mod_Rewrite_Shop/Detailsweb-camera-a4tech2\n",
      "HTTPError: 404\n",
      "Scraping from http://testphp.vulnweb.com/Mod_Rewrite_Shop/Detailsnetwork-attached-storage-dlink1\n",
      "HTTPError: 404\n",
      "Scraping from http://testphp.vulnweb.com/Mod_Rewrite_Shop/Detailscolor-printer3\n",
      "HTTPError: 404\n"
     ]
    }
   ],
   "source": [
    "url_list = s.scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://testphp.vulnweb.com\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/userinfo.php\n",
      "[] ['uname', 'pass']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/login.php\n",
      "[] ['uname', 'pass']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/artists.php\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/cart.php\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/guestbook.php\n",
      "[] ['name', 'submit']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/disclaimer.php\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/categories.php\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/index.php\n",
      "[] ['searchFor', 'goButton']\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/artists.php/artists.php\n",
      "['artist'] []\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/categories.php/listproducts.php\n",
      "['cat'] []\n",
      "-----------------------\n",
      "http://testphp.vulnweb.com/hpp/\n",
      "['pp'] []\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in url_list:\n",
    "    print(i.url)\n",
    "    print(i.getparams, i.postparams)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from http://www.testfire.net\n",
      "Scraping from http://www.testfire.net/cgi.exe\n",
      "HTTPError: 404\n",
      "Scraping from http://www.testfire.net/subscribe.jsp\n",
      "Scraping from http://www.testfire.net/login.jsp\n",
      "Scraping from http://www.testfire.net/index.jsp\n",
      "Scraping from http://www.testfire.net/swaggerindex.html\n",
      "HTTPError: 404\n",
      "Scraping from http://www.testfire.net/feedback.jsp\n",
      "Scraping from http://www.testfire.net/survey_questions.jsp\n",
      "Scraping from http://www.testfire.net/status_check.jsp\n"
     ]
    }
   ],
   "source": [
    "c=ScrapedURLs(\"http://www.testfire.net\")\n",
    "url_list2 = c.scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.testfire.net\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/subscribe.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/login.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/index.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/feedback.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/survey_questions.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/status_check.jsp\n",
      "[] ['query']\n",
      "-----------------------\n",
      "http://www.testfire.net/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/default.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/subscribe.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/login.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/index.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/index.jsp/default.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/feedback.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/survey_questions.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/survey_questions.jsp/survey_questions.jsp\n",
      "['step'] []\n",
      "-----------------------\n",
      "http://www.testfire.net/status_check.jsp/index.jsp\n",
      "['content'] []\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in url_list2:\n",
    "    print(i.url)\n",
    "    print(i.getparams, i.postparams)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen(\"http://testphp.vulnweb.com/artists.php\")\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = soup.find_all('form')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfuzz -c -z file,./wordlist/Injections/SQL.txt -d \"searchFor=FUZZ&goButton=go\" --hc 404 http://testphp.vulnweb.com/search.php\n",
    "wfuzz -z file,wordlist/others/common_pass.txt -d \"uname=FUZZ&pass=FUZZ\"  --hc 302 http://testphp.vulnweb.com/userinfo.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
